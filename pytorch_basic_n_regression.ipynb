{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# ------------ 1. 파이토치 기초 -------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 01. 파이토치 패키지의 기본 구성\n",
    "### 1. torch\n",
    "메인 네임스페이스. 텐서 등의 다양한 수학 함수가 포함되어져 있으며 Numpy와 유사한 구조를 가집니다.\n",
    "### 2. torch.autograd\n",
    "자동 미분을 위한 함수들이 포함되어져 있습니다. 자동 미분의 on/off를 제어하는 콘텍스트 매니저 enable_grad, no_grad, 자체 미분 가능 함수를 정의할 때 사용하는 기반 클래스인 Function등이 포함되어져 있습니다.\n",
    "### 3. torch.nn\n",
    "신경망을 구축하기 위한 다양한 데이터 구조나 레이어 등이 정의되어져 있습니다. 예를 들어 RNN, LSTM과 같은 레이어, ReLU같은 활성화 함수, MSELoss와 같은 손실 함수들이 있습니다.\n",
    "### 4. torch.optim\n",
    "확률적 경사 하강법(Stochasitic Gradient Descent, SGD)을 중심으로 한 파라미터 최적화 알고리즘이 구현되어져 있습니다.\n",
    "### 5. torch.utils.data\n",
    "### 6. torch.onnx"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "## 02. 텐서 조작하기 1\n",
    "#### batch size : 훈련 데이터에 개수가 많을 때, 컴퓨터가 한번에 처리하는 양(배치 크기)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6.]\n1\n(7,)\ntensor([0., 1., 2., 3., 4., 5., 6.])\n1\ntorch.Size([7])\ntorch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "# Numpy를 Tensor로 만들기\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "t = np.array([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)\n",
    "print(t.ndim) # 1차원\n",
    "print(t.shape) # (1 X 7)의 크기를 가지는 벡터 (7, 1)\n",
    "\n",
    "# 2행 3열의 2D 텐서 : (2, 3), (2 X 3)\n",
    "\n",
    "import torch\n",
    "t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)\n",
    "print(t.dim())\n",
    "print(t.shape)\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[4., 5.]])\ntensor([[1., 2.],\n        [3., 4.]])\ntensor([[1.],\n        [2.]])\ntensor([[ 5.],\n        [11.]])\ntensor([[1., 2.],\n        [6., 8.]])\ntensor([[1., 2.],\n        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# 크기가 다른 tensor간의 연산 : 브로드캐스팅(BroadCasting)\n",
    "m1 = torch.FloatTensor([[1,2]])\n",
    "m2 = torch.FloatTensor([3])\n",
    "print(m1+m2)\n",
    "\n",
    "# 행렬 곱셈\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print(m1)\n",
    "print(m2)\n",
    "print(m1.matmul(m2))\n",
    "\n",
    "# element-wise 곱셈\n",
    "print(m1.mul(m2))\n",
    "print(m1 * m2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 2.],\n        [3., 4.]])\ntensor(2.5000)\ntensor([4., 6.])\ntensor([2., 4.])\ntensor([2., 4.])\ntensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Tensor의 평균/덧셈/max/argmax\n",
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)\n",
    "print(t.mean())\n",
    "print(t.sum(dim=0)) # dim=0 : 첫번째 차원, 행을 제거한다는 의미, 즉, 열의 차원을 보존하며 평균을 구함 -> (1,2)\n",
    "print(t.max(dim=1)[0]) #  -> (2,1) 하지만 결국 1차원이므로 (1,2)와 같이 표현됨\n",
    "print(t.max(dim=-1)[0]) # max : 최대값을 리턴, 마지막 차원 제거\n",
    "print(t.max(dim=1)[1]) # argmax : 최대값을 가진 인덱스를 리턴"
   ]
  },
  {
   "source": [
    "\n",
    "## 03. 텐서 조작하기 2\n",
    "#### 뷰(view) : 원소의 수를 유치하면서 텐서의 크기 변경. 매우 중요함!! numpy에서의 reshape"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[ 0.,  1.,  2.],\n         [ 3.,  4.,  5.]],\n\n        [[ 6.,  7.,  8.],\n         [ 9., 10., 11.]]])\ntorch.Size([2, 2, 3])\ntensor([[ 0.,  1.,  2.],\n        [ 3.,  4.,  5.],\n        [ 6.,  7.,  8.],\n        [ 9., 10., 11.]])\ntorch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "              [[6, 7, 8],\n",
    "               [9, 10, 11]]])\n",
    "ft = torch.FloatTensor(t)\n",
    "print(ft)\n",
    "print(ft.shape)\n",
    "print(ft.view(-1,3))\n",
    "print(ft.view(-1,3).shape) # 텐서를 (?,3)의 크기로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.],\n        [1.],\n        [2.]])\ntorch.Size([3, 1])\ntensor([0., 1., 2.])\ntorch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# squeeze : 크기가 1인 차원을 제거합니다.\n",
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft)\n",
    "print(ft.shape)\n",
    "\n",
    "sq = ft.squeeze()\n",
    "\n",
    "print(sq)\n",
    "print(sq.shape)\n",
    "\n",
    "# unsqueeze : 특정 위치에 1인 차원을 추가한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0., 1., 2.]])\ntorch.Size([1, 3])\ntensor([[0.],\n        [1.],\n        [2.]])\ntensor([[0.],\n        [1.],\n        [2.]])\n"
     ]
    }
   ],
   "source": [
    "print(sq.unsqueeze(0)) # 인덱스가 0부터 시작하므로 0은 첫번째 차원을 의미한다.\n",
    "print(sq.unsqueeze(0).shape)\n",
    "\n",
    "print(sq.unsqueeze(1)) # 두번째 차원에 추가\n",
    "print(sq.unsqueeze(-1)) # 마지막 차원에 1인 차원 추가"
   ]
  },
  {
   "source": [
    "#### Tensor의 자료형 : Type Casting\n",
    "`torch.float32` `torch.float` `torch.double` `torch.long` `torch.int64` 등등"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 2.],\n        [3., 4.],\n        [5., 6.],\n        [7., 8.]])\ntensor([[1., 2., 5., 6.],\n        [3., 4., 7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# tensor 연결하기\n",
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "y = torch.FloatTensor([[5, 6], [7, 8]])\n",
    "\n",
    "print(torch.cat([x,y], dim=0)) # 첫번째 차원을 늘릴것\n",
    "print(torch.cat([x,y], dim=1)) # 두번째 차원을 늘릴것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 4.],\n        [2., 5.],\n        [3., 6.]])\ntensor([[1., 2., 3.],\n        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# stacking\n",
    "x = torch.FloatTensor([1, 4])\n",
    "y = torch.FloatTensor([2, 5])\n",
    "z = torch.FloatTensor([3, 6])\n",
    "\n",
    "print(torch.stack([x, y, z]))\n",
    "\n",
    "print(torch.stack([x, y, z], dim=1)) # 두번째 차원이 증가하도록 쌓으라는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 동일한 크기를 갖는 텐서 만들기\n",
    "x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\n",
    "print(torch.ones_like(x)) # 입력 텐서와 크기를 동일하게 하면서 값을 1로 채우기\n",
    "print(torch.zeros_like(x)) # 입력 텐서와 크기를 동일하게 하면서 값을 0으로 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 2.],\n        [3., 4.]])\ntensor([[2., 4.],\n        [6., 8.]])\ntensor([[2., 4.],\n        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# In-place Operation(덮어쓰기 연산) : 연산 뒤에 _를 붙이면 기존의 값을 덮어쓰기함.\n",
    "x = torch.FloatTensor([[1,2],[3,4]])\n",
    "print(x)\n",
    "print(x.mul_(2.))\n",
    "print(x)"
   ]
  },
  {
   "source": [
    "## 04. 파이썬 : 클래스(class)\n",
    "#### 빵틀과 빵, 코드를 간결하게\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# ------------ 2. 선형 회귀 --------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 01. 선형 회귀\n",
    "#### 데이터에 대한 이해, 가설 수립, 손실 계산하기, 경사 하강법"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16aad769bf0>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 1])\ntorch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[4], [5], [6]])\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "source": [
    "$ y = Wx + b $ <br><br>\n",
    "가장 잘 맞는 직선을 정의하는 W와 b의 값을 찾는 것."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.], requires_grad=True)\ntensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.],\n        [0.],\n        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(25.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis-y_train)**2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01) # 학습 대상 : W, b, 학습률 = 0.01\n",
    "optimizer.zero_grad() # gradient를 0으로 초기화, 파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있습니다. 따라서 기울기를 초기화해야만 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있습니다.\n",
    "\n",
    "cost.backward() #비용함수를 미분하여 gradient 계산\n",
    "\n",
    "optimizer.step() # 리턴되는 변수들의 gradient에 학습률 0.01을 곱하여 빼주면서 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/2000 W: 0.403, b: 0.189 Cost: 20.423275\n",
      "Epoch  100/2000 W: 1.738, b: 1.322 Cost: 0.405913\n",
      "Epoch  200/2000 W: 1.580, b: 1.681 Cost: 0.250829\n",
      "Epoch  300/2000 W: 1.456, b: 1.963 Cost: 0.154997\n",
      "Epoch  400/2000 W: 1.359, b: 2.185 Cost: 0.095779\n",
      "Epoch  500/2000 W: 1.282, b: 2.359 Cost: 0.059185\n",
      "Epoch  600/2000 W: 1.222, b: 2.496 Cost: 0.036573\n",
      "Epoch  700/2000 W: 1.174, b: 2.604 Cost: 0.022600\n",
      "Epoch  800/2000 W: 1.137, b: 2.689 Cost: 0.013965\n",
      "Epoch  900/2000 W: 1.108, b: 2.755 Cost: 0.008630\n",
      "Epoch 1000/2000 W: 1.085, b: 2.808 Cost: 0.005333\n",
      "Epoch 1100/2000 W: 1.067, b: 2.849 Cost: 0.003295\n",
      "Epoch 1200/2000 W: 1.052, b: 2.881 Cost: 0.002036\n",
      "Epoch 1300/2000 W: 1.041, b: 2.907 Cost: 0.001258\n",
      "Epoch 1400/2000 W: 1.032, b: 2.927 Cost: 0.000778\n",
      "Epoch 1500/2000 W: 1.025, b: 2.942 Cost: 0.000480\n",
      "Epoch 1600/2000 W: 1.020, b: 2.955 Cost: 0.000297\n",
      "Epoch 1700/2000 W: 1.016, b: 2.964 Cost: 0.000183\n",
      "Epoch 1800/2000 W: 1.012, b: 2.972 Cost: 0.000113\n",
      "Epoch 1900/2000 W: 1.010, b: 2.978 Cost: 0.000070\n",
      "Epoch 2000/2000 W: 1.008, b: 2.983 Cost: 0.000043\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "source": [
    "## 02. 자동 미분(Autograd)\n",
    "#### 경사 하강법은 비용 함수를 미분하여 이 함수의 기울기(gradient)를 구해서 비용이 최소화 되는 방향을 찾아내는 알고리즘, 파이토치는 자동 미분(Autograd)을 지원"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 03. 다중 선형 회귀\n",
    "#### 다수의 x로부터 y를 예측"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16aad769bf0>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "source": [
    "### 안좋은 코드, 각 매개변수만큼 tensor 선언"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563634\n",
      "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497607\n",
      "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435026\n",
      "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375730\n",
      "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319511\n",
      "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215696\n",
      "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167818\n",
      "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079378\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "source": [
    "### 더 나은 코드, 행렬곱 사용"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch  100/1000 hypothesis: tensor([152.7691, 183.6985, 180.9591, 197.0627, 140.1336]) Cost: 1.563634\n",
      "Epoch  200/1000 hypothesis: tensor([152.7273, 183.7273, 180.9465, 197.0517, 140.1731]) Cost: 1.497607\n",
      "Epoch  300/1000 hypothesis: tensor([152.6866, 183.7554, 180.9343, 197.0409, 140.2116]) Cost: 1.435026\n",
      "Epoch  400/1000 hypothesis: tensor([152.6470, 183.7827, 180.9224, 197.0304, 140.2491]) Cost: 1.375730\n",
      "Epoch  500/1000 hypothesis: tensor([152.6085, 183.8093, 180.9108, 197.0201, 140.2856]) Cost: 1.319511\n",
      "Epoch  600/1000 hypothesis: tensor([152.5711, 183.8352, 180.8996, 197.0101, 140.3211]) Cost: 1.266222\n",
      "Epoch  700/1000 hypothesis: tensor([152.5346, 183.8604, 180.8887, 197.0003, 140.3558]) Cost: 1.215696\n",
      "Epoch  800/1000 hypothesis: tensor([152.4992, 183.8849, 180.8781, 196.9908, 140.3895]) Cost: 1.167818\n",
      "Epoch  900/1000 hypothesis: tensor([152.4647, 183.9087, 180.8677, 196.9814, 140.4223]) Cost: 1.122429\n",
      "Epoch 1000/1000 hypothesis: tensor([152.4312, 183.9319, 180.8577, 196.9723, 140.4543]) Cost: 1.079378\n"
     ]
    }
   ],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "\n",
    "# 가중치와 편향 선언\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "        ))\n"
   ]
  },
  {
   "source": [
    "## 04. `nn.Module` 로 구현하는 선형 회귀\n",
    "#### 지금 까지는 직접적인 이해를 위해 가설, 비용 함수를 직접 정의해서 구현했음\n",
    "#### nn.Module을 이용해 이미 구현되어져 제공되고 있는 함수들을 불러오는 것으로 더 쉽게 구현\n",
    "#### 선형 회귀 모델 \n",
    "```\n",
    "import torch.nn as nn\n",
    "model = nn.Linear(input_dim, output_dim)\n",
    "```\n",
    "#### 평균 제곱오차\n",
    "```\n",
    "import torch.nn.functional as F\n",
    "cost = F.mse_loss(prediction, y_train)\n",
    "```\n",
    "H(x) 식에 입력 x로부터 예측된 y를 얻는 것을 forward 연산이라고 합니다.<br>\n",
    "학습 전, prediction = model(x_train)은 x_train으로부터 예측값을 리턴하므로 forward 연산입니다.<br>\n",
    "학습 후, pred_y = model(new_var)는 임의의 값 new_var로부터 예측값을 리턴하므로 forward 연산입니다.<br>\n",
    "학습 과정에서 비용 함수를 미분하여 기울기를 구하는 것을 backward 연산이라고 합니다.<br>\n",
    "cost.backward()는 비용 함수로부터 기울기를 구하라는 의미이며 backward 연산입니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.1939]], requires_grad=True), Parameter containing:\n",
      "tensor([0.4694], requires_grad=True)]\n",
      "Epoch    0/2000 Cost: 18.562185\n",
      "Epoch  100/2000 Cost: 0.128051\n",
      "Epoch  200/2000 Cost: 0.079128\n",
      "Epoch  300/2000 Cost: 0.048896\n",
      "Epoch  400/2000 Cost: 0.030215\n",
      "Epoch  500/2000 Cost: 0.018671\n",
      "Epoch  600/2000 Cost: 0.011538\n",
      "Epoch  700/2000 Cost: 0.007129\n",
      "Epoch  800/2000 Cost: 0.004406\n",
      "Epoch  900/2000 Cost: 0.002722\n",
      "Epoch 1000/2000 Cost: 0.001682\n",
      "Epoch 1100/2000 Cost: 0.001040\n",
      "Epoch 1200/2000 Cost: 0.000642\n",
      "Epoch 1300/2000 Cost: 0.000397\n",
      "Epoch 1400/2000 Cost: 0.000245\n",
      "Epoch 1500/2000 Cost: 0.000152\n",
      "Epoch 1600/2000 Cost: 0.000094\n",
      "Epoch 1700/2000 Cost: 0.000058\n",
      "Epoch 1800/2000 Cost: 0.000036\n",
      "Epoch 1900/2000 Cost: 0.000022\n",
      "Epoch 2000/2000 Cost: 0.000014\n"
     ]
    }
   ],
   "source": [
    "# 단순 선형 회귀, 훨씬 낮은 cost\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "model = nn.Linear(1,1) # input_dim : 1, output_dim : 1\n",
    "print(list(model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward() # backward 연산\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Parameter containing:\ntensor([[1.9957]], requires_grad=True), Parameter containing:\ntensor([0.0097], requires_grad=True)]\ntensor([7.9926], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 후의 W와 b의 값을 출력\n",
    "print(list(model.parameters()))\n",
    "# x에 4를 넣어 y 값 확인\n",
    "new_var = torch.FloatTensor([4.0])\n",
    "pred_y = model(new_var)\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/2000 Cost: 31362.457031\n",
      "Epoch  100/2000 Cost: 0.267423\n",
      "Epoch  200/2000 Cost: 0.266820\n",
      "Epoch  300/2000 Cost: 0.266232\n",
      "Epoch  400/2000 Cost: 0.265654\n",
      "Epoch  500/2000 Cost: 0.265084\n",
      "Epoch  600/2000 Cost: 0.264517\n",
      "Epoch  700/2000 Cost: 0.263968\n",
      "Epoch  800/2000 Cost: 0.263417\n",
      "Epoch  900/2000 Cost: 0.262872\n",
      "Epoch 1000/2000 Cost: 0.262339\n",
      "Epoch 1100/2000 Cost: 0.261804\n",
      "Epoch 1200/2000 Cost: 0.261291\n",
      "Epoch 1300/2000 Cost: 0.260769\n",
      "Epoch 1400/2000 Cost: 0.260260\n",
      "Epoch 1500/2000 Cost: 0.259756\n",
      "Epoch 1600/2000 Cost: 0.259251\n",
      "Epoch 1700/2000 Cost: 0.258757\n",
      "Epoch 1800/2000 Cost: 0.258259\n",
      "Epoch 1900/2000 Cost: 0.257775\n",
      "Epoch 2000/2000 Cost: 0.257304\n"
     ]
    }
   ],
   "source": [
    "#다중 선형 회귀 분석\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "model = nn.Linear(3,1)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    cost = F.mse_loss(prediction, y_train) \n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[151.3034]], grad_fn=<AddmmBackward>)\n[Parameter containing:\ntensor([[0.9083, 0.4603, 0.6429]], requires_grad=True), Parameter containing:\ntensor([-0.0421], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "pred_y = model(new_var) \n",
    "print(pred_y)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "source": [
    "## 05. 클래스로 파이토치 모델 구현하기\n",
    "#### 파이토치는 대부분의 모델을 클래스를 사용해서 구분합니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16aad769bf0>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/2000 Cost: 13.103541\n",
      "Epoch  100/2000 Cost: 0.002791\n",
      "Epoch  200/2000 Cost: 0.001724\n",
      "Epoch  300/2000 Cost: 0.001066\n",
      "Epoch  400/2000 Cost: 0.000658\n",
      "Epoch  500/2000 Cost: 0.000407\n",
      "Epoch  600/2000 Cost: 0.000251\n",
      "Epoch  700/2000 Cost: 0.000155\n",
      "Epoch  800/2000 Cost: 0.000096\n",
      "Epoch  900/2000 Cost: 0.000059\n",
      "Epoch 1000/2000 Cost: 0.000037\n",
      "Epoch 1100/2000 Cost: 0.000023\n",
      "Epoch 1200/2000 Cost: 0.000014\n",
      "Epoch 1300/2000 Cost: 0.000009\n",
      "Epoch 1400/2000 Cost: 0.000005\n",
      "Epoch 1500/2000 Cost: 0.000003\n",
      "Epoch 1600/2000 Cost: 0.000002\n",
      "Epoch 1700/2000 Cost: 0.000001\n",
      "Epoch 1800/2000 Cost: 0.000001\n",
      "Epoch 1900/2000 Cost: 0.000000\n",
      "Epoch 2000/2000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 선형 회귀 클래스로 구현하기\n",
    "# 대부분의 파이토치 구현체에서 사용하고 있는 방식으로 반드시 숙지할 필요가 있습니다.\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "# model = nn.Linear(1,1)를 클래스로 구현하면,\n",
    "# -----------------------------------\n",
    "class LinearRegressionModel(nn.Module): # nn.Module을 상속받습니다.\n",
    "    def __init__(self): \n",
    "        super().__init__() #nn.Module 클래스의 속성들을 가지고 초기화 됩니다.\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x): # model 객체를 데이터와 함께 호출하면 자동으로 실행됩니다.\n",
    "        return self.linear(x)\n",
    "model = LinearRegressionModel()\n",
    "# -----------------------------------\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    cost.backward() \n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "source": [
    "## 06. 미니 배치와 데이터 로드\n",
    "#### 현업에서의 방대한 데이터에서 경사 하강법을 수행하는 것은 매우 느리며 많은 계산을 필요로 합니다. <br> 그렇기 때문에 전체 데이터를 더 작은 단위로 나누어서 해당 단위로 학습하는 개념이 나타났습니다.\n",
    "#### epoch : *전체* 훈련 데이터가 학습에 한 번 사용 된 주기\n",
    "#### batch size : 배치 크기는 보통 2의 제곱수를 사용\n",
    "#### iteration : 한 번의 에포크 내에서 이루어지는 매개변수인 가중치 W와 b의 업데이트 횟수\n",
    "#### 데이터 로드하기 : `Dataset`, `DataLoader` : 미니 배치 학습, 셔플, 병렬 처리까지 간단히 수행합니다.<br> 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것입니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 89.,  91.,  90.]]), tensor([[196.],\n",
      "        [180.]])]\n",
      "Epoch    0/20 Batch 1/3 Cost: 50294.707031\n",
      "1\n",
      "[tensor([[73., 80., 75.],\n",
      "        [73., 66., 70.]]), tensor([[152.],\n",
      "        [142.]])]\n",
      "Epoch    0/20 Batch 2/3 Cost: 6763.595703\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch    0/20 Batch 3/3 Cost: 5374.647949\n",
      "0\n",
      "[tensor([[89., 91., 90.],\n",
      "        [93., 88., 93.]]), tensor([[180.],\n",
      "        [185.]])]\n",
      "Epoch    1/20 Batch 1/3 Cost: 1162.977417\n",
      "1\n",
      "[tensor([[73., 80., 75.],\n",
      "        [73., 66., 70.]]), tensor([[152.],\n",
      "        [142.]])]\n",
      "Epoch    1/20 Batch 2/3 Cost: 183.729141\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch    1/20 Batch 3/3 Cost: 137.012070\n",
      "0\n",
      "[tensor([[73., 66., 70.],\n",
      "        [93., 88., 93.]]), tensor([[142.],\n",
      "        [185.]])]\n",
      "Epoch    2/20 Batch 1/3 Cost: 57.777779\n",
      "1\n",
      "[tensor([[ 73.,  80.,  75.],\n",
      "        [ 96.,  98., 100.]]), tensor([[152.],\n",
      "        [196.]])]\n",
      "Epoch    2/20 Batch 2/3 Cost: 2.246849\n",
      "2\n",
      "[tensor([[89., 91., 90.]]), tensor([[180.]])]\n",
      "Epoch    2/20 Batch 3/3 Cost: 0.000757\n",
      "0\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 80., 75.]]), tensor([[180.],\n",
      "        [152.]])]\n",
      "Epoch    3/20 Batch 1/3 Cost: 1.395439\n",
      "1\n",
      "[tensor([[ 73.,  66.,  70.],\n",
      "        [ 96.,  98., 100.]]), tensor([[142.],\n",
      "        [196.]])]\n",
      "Epoch    3/20 Batch 2/3 Cost: 12.437510\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch    3/20 Batch 3/3 Cost: 13.789183\n",
      "0\n",
      "[tensor([[73., 80., 75.],\n",
      "        [73., 66., 70.]]), tensor([[152.],\n",
      "        [142.]])]\n",
      "Epoch    4/20 Batch 1/3 Cost: 10.726515\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [93., 88., 93.]]), tensor([[180.],\n",
      "        [185.]])]\n",
      "Epoch    4/20 Batch 2/3 Cost: 5.437060\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch    4/20 Batch 3/3 Cost: 1.679471\n",
      "0\n",
      "[tensor([[73., 66., 70.],\n",
      "        [89., 91., 90.]]), tensor([[142.],\n",
      "        [180.]])]\n",
      "Epoch    5/20 Batch 1/3 Cost: 6.268850\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 93.,  88.,  93.]]), tensor([[196.],\n",
      "        [185.]])]\n",
      "Epoch    5/20 Batch 2/3 Cost: 4.210349\n",
      "2\n",
      "[tensor([[73., 80., 75.]]), tensor([[152.]])]\n",
      "Epoch    5/20 Batch 3/3 Cost: 13.457353\n",
      "0\n",
      "[tensor([[89., 91., 90.],\n",
      "        [93., 88., 93.]]), tensor([[180.],\n",
      "        [185.]])]\n",
      "Epoch    6/20 Batch 1/3 Cost: 7.691123\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]]), tensor([[196.],\n",
      "        [142.]])]\n",
      "Epoch    6/20 Batch 2/3 Cost: 5.167531\n",
      "2\n",
      "[tensor([[73., 80., 75.]]), tensor([[152.]])]\n",
      "Epoch    6/20 Batch 3/3 Cost: 11.714764\n",
      "0\n",
      "[tensor([[93., 88., 93.],\n",
      "        [73., 66., 70.]]), tensor([[185.],\n",
      "        [142.]])]\n",
      "Epoch    7/20 Batch 1/3 Cost: 15.647712\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 89.,  91.,  90.]]), tensor([[196.],\n",
      "        [180.]])]\n",
      "Epoch    7/20 Batch 2/3 Cost: 3.712581\n",
      "2\n",
      "[tensor([[73., 80., 75.]]), tensor([[152.]])]\n",
      "Epoch    7/20 Batch 3/3 Cost: 8.434143\n",
      "0\n",
      "[tensor([[89., 91., 90.],\n",
      "        [93., 88., 93.]]), tensor([[180.],\n",
      "        [185.]])]\n",
      "Epoch    8/20 Batch 1/3 Cost: 9.779198\n",
      "1\n",
      "[tensor([[73., 66., 70.],\n",
      "        [73., 80., 75.]]), tensor([[142.],\n",
      "        [152.]])]\n",
      "Epoch    8/20 Batch 2/3 Cost: 9.610333\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch    8/20 Batch 3/3 Cost: 0.040170\n",
      "0\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 80., 75.]]), tensor([[180.],\n",
      "        [152.]])]\n",
      "Epoch    9/20 Batch 1/3 Cost: 4.522668\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 93.,  88.,  93.]]), tensor([[196.],\n",
      "        [185.]])]\n",
      "Epoch    9/20 Batch 2/3 Cost: 9.546425\n",
      "2\n",
      "[tensor([[73., 66., 70.]]), tensor([[142.]])]\n",
      "Epoch    9/20 Batch 3/3 Cost: 9.640429\n",
      "0\n",
      "[tensor([[ 73.,  80.,  75.],\n",
      "        [ 96.,  98., 100.]]), tensor([[152.],\n",
      "        [196.]])]\n",
      "Epoch   10/20 Batch 1/3 Cost: 9.910338\n",
      "1\n",
      "[tensor([[93., 88., 93.],\n",
      "        [73., 66., 70.]]), tensor([[185.],\n",
      "        [142.]])]\n",
      "Epoch   10/20 Batch 2/3 Cost: 10.056160\n",
      "2\n",
      "[tensor([[89., 91., 90.]]), tensor([[180.]])]\n",
      "Epoch   10/20 Batch 3/3 Cost: 8.437865\n",
      "0\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 66., 70.]]), tensor([[180.],\n",
      "        [142.]])]\n",
      "Epoch   11/20 Batch 1/3 Cost: 6.366612\n",
      "1\n",
      "[tensor([[93., 88., 93.],\n",
      "        [73., 80., 75.]]), tensor([[185.],\n",
      "        [152.]])]\n",
      "Epoch   11/20 Batch 2/3 Cost: 9.140144\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch   11/20 Batch 3/3 Cost: 0.477157\n",
      "0\n",
      "[tensor([[ 73.,  80.,  75.],\n",
      "        [ 96.,  98., 100.]]), tensor([[152.],\n",
      "        [196.]])]\n",
      "Epoch   12/20 Batch 1/3 Cost: 4.204929\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 66., 70.]]), tensor([[180.],\n",
      "        [142.]])]\n",
      "Epoch   12/20 Batch 2/3 Cost: 7.485415\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch   12/20 Batch 3/3 Cost: 11.228778\n",
      "0\n",
      "[tensor([[73., 66., 70.],\n",
      "        [93., 88., 93.]]), tensor([[142.],\n",
      "        [185.]])]\n",
      "Epoch   13/20 Batch 1/3 Cost: 3.610157\n",
      "1\n",
      "[tensor([[ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.]]), tensor([[180.],\n",
      "        [196.]])]\n",
      "Epoch   13/20 Batch 2/3 Cost: 11.034501\n",
      "2\n",
      "[tensor([[73., 80., 75.]]), tensor([[152.]])]\n",
      "Epoch   13/20 Batch 3/3 Cost: 11.799837\n",
      "0\n",
      "[tensor([[73., 66., 70.],\n",
      "        [89., 91., 90.]]), tensor([[142.],\n",
      "        [180.]])]\n",
      "Epoch   14/20 Batch 1/3 Cost: 7.651086\n",
      "1\n",
      "[tensor([[73., 80., 75.],\n",
      "        [93., 88., 93.]]), tensor([[152.],\n",
      "        [185.]])]\n",
      "Epoch   14/20 Batch 2/3 Cost: 9.453345\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch   14/20 Batch 3/3 Cost: 0.146604\n",
      "0\n",
      "[tensor([[ 73.,  66.,  70.],\n",
      "        [ 96.,  98., 100.]]), tensor([[142.],\n",
      "        [196.]])]\n",
      "Epoch   15/20 Batch 1/3 Cost: 5.644117\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 80., 75.]]), tensor([[180.],\n",
      "        [152.]])]\n",
      "Epoch   15/20 Batch 2/3 Cost: 7.190745\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch   15/20 Batch 3/3 Cost: 15.003801\n",
      "0\n",
      "[tensor([[73., 66., 70.],\n",
      "        [73., 80., 75.]]), tensor([[142.],\n",
      "        [152.]])]\n",
      "Epoch   16/20 Batch 1/3 Cost: 10.337284\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [93., 88., 93.]]), tensor([[180.],\n",
      "        [185.]])]\n",
      "Epoch   16/20 Batch 2/3 Cost: 5.349921\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch   16/20 Batch 3/3 Cost: 1.552598\n",
      "0\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  80.,  75.]]), tensor([[196.],\n",
      "        [152.]])]\n",
      "Epoch   17/20 Batch 1/3 Cost: 4.806367\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 66., 70.]]), tensor([[180.],\n",
      "        [142.]])]\n",
      "Epoch   17/20 Batch 2/3 Cost: 7.181715\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch   17/20 Batch 3/3 Cost: 10.670586\n",
      "0\n",
      "[tensor([[89., 91., 90.],\n",
      "        [93., 88., 93.]]), tensor([[180.],\n",
      "        [185.]])]\n",
      "Epoch   18/20 Batch 1/3 Cost: 5.774303\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  80.,  75.]]), tensor([[196.],\n",
      "        [152.]])]\n",
      "Epoch   18/20 Batch 2/3 Cost: 8.880335\n",
      "2\n",
      "[tensor([[73., 66., 70.]]), tensor([[142.]])]\n",
      "Epoch   18/20 Batch 3/3 Cost: 10.642990\n",
      "0\n",
      "[tensor([[73., 66., 70.],\n",
      "        [73., 80., 75.]]), tensor([[142.],\n",
      "        [152.]])]\n",
      "Epoch   19/20 Batch 1/3 Cost: 10.243836\n",
      "1\n",
      "[tensor([[ 93.,  88.,  93.],\n",
      "        [ 96.,  98., 100.]]), tensor([[185.],\n",
      "        [196.]])]\n",
      "Epoch   19/20 Batch 2/3 Cost: 3.436625\n",
      "2\n",
      "[tensor([[89., 91., 90.]]), tensor([[180.]])]\n",
      "Epoch   19/20 Batch 3/3 Cost: 6.480822\n",
      "0\n",
      "[tensor([[93., 88., 93.],\n",
      "        [89., 91., 90.]]), tensor([[185.],\n",
      "        [180.]])]\n",
      "Epoch   20/20 Batch 1/3 Cost: 6.376135\n",
      "1\n",
      "[tensor([[ 73.,  66.,  70.],\n",
      "        [ 96.,  98., 100.]]), tensor([[142.],\n",
      "        [196.]])]\n",
      "Epoch   20/20 Batch 2/3 Cost: 4.632808\n",
      "2\n",
      "[tensor([[73., 80., 75.]]), tensor([[152.]])]\n",
      "Epoch   20/20 Batch 3/3 Cost: 12.166296\n"
     ]
    }
   ],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "\n",
    "dataset = TensorDataset(x_train, y_train) \n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "# batch_size : 미니 배치의 크기\n",
    "# shuffle=True : Epoch마다 데이터셋을 섞어서 데이터가 학습되는 순서를 바꿉니다.(권장)\n",
    "\n",
    "model = nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "  for batch_idx, samples in enumerate(dataloader):\n",
    "    print(batch_idx)\n",
    "    print(samples)\n",
    "    x_train, y_train = samples\n",
    "\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))"
   ]
  },
  {
   "source": [
    "## 07. 커스텀 데이터셋\n",
    "#### `torch.utils.data.Dataset`을 상속받아 직접 커스텀 데이터셋을 만드는 경우\n",
    "```\n",
    "class CustomDataset(torch.utils.data.Dataset): \n",
    "  def __init__(self):\n",
    "  데이터셋의 전처리를 해주는 부분\n",
    "\n",
    "  def __len__(self):\n",
    "  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "\n",
    "  def __getitem__(self, idx): \n",
    "  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dataset 상속\n",
    "class CustomDataset(Dataset): \n",
    "  def __init__(self):\n",
    "    self.x_data = [[73, 80, 75],\n",
    "                   [93, 88, 93],\n",
    "                   [89, 91, 90],\n",
    "                   [96, 98, 100],\n",
    "                   [73, 66, 70]]\n",
    "    self.y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "  # 총 데이터의 개수를 리턴\n",
    "  def __len__(self): \n",
    "    return len(self.x_data)\n",
    "\n",
    "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "  def __getitem__(self, idx): \n",
    "    x = torch.FloatTensor(self.x_data[idx])\n",
    "    y = torch.FloatTensor(self.y_data[idx])\n",
    "    return x, y\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "source": [
    "# ------------ 4. 로지스틱 회귀 -------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 01. 로지스틱 회귀\n",
    "#### 이진 분류(Binary Classification)의 대표적인 알고리즘\n",
    "#### 시그모이드 함수 $$ H(x) = sigmoid(Wx+b) = {1 \\over 1 + exp(-(Wx+b))} = \\sigma(Wx+b) $$\n",
    "#### 시그모이드 함수의 출력값은 0과 1사이의 값을 가집니다.\n",
    "#### 실제값이 1일 때 예측값이 0에 가까워지면, 실제값이 0일때 예측값이 1에 가까워지면 오차가 커져야 합니다. -> 로그 함수 사용\n",
    "#### 실제값이 1일 때 $−logH(x)$ 그래프를 사용하고 y의 실제값이 0일 때 $−log(1−H(X))$ 그래프를 사용해야 합니다.\n",
    "#### $$ cost(H(x), y) = -[ ylogH(x) + (1-y)log(1-H(x)) ]$$\n",
    "#### $$ cost(W)=−{1 \\over n}\\sum\\limits_{i=1}^n[y^{(i)}logH(x^{(i)})+(1−y^{(i)})log(1−H(x^{(i)}))] $$\n",
    "#### $$ W := W-\\alpha{\\partial \\over \\partial W}cost(W) $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 372.103125 263.63625\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2020-12-28T21:31:15.716766</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 263.63625 \r\nL 372.103125 263.63625 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 239.758125 \r\nL 364.903125 239.758125 \r\nL 364.903125 22.318125 \r\nL 30.103125 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m4f1682eca7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"76.065108\" xlink:href=\"#m4f1682eca7\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- −4 -->\r\n      <g transform=\"translate(68.694015 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.59375 35.5 \r\nL 73.1875 35.5 \r\nL 73.1875 27.203125 \r\nL 10.59375 27.203125 \r\nz\r\n\" id=\"DejaVuSans-8722\"/>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"137.552712\" xlink:href=\"#m4f1682eca7\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- −2 -->\r\n      <g transform=\"translate(130.181618 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"199.040315\" xlink:href=\"#m4f1682eca7\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(195.859065 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"260.527918\" xlink:href=\"#m4f1682eca7\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(257.346668 254.356563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"322.015522\" xlink:href=\"#m4f1682eca7\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(318.834272 254.356563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_6\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m1f9a9d69ec\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m1f9a9d69ec\" y=\"229.874489\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(7.2 233.673707)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m1f9a9d69ec\" y=\"190.339943\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 194.139162)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m1f9a9d69ec\" y=\"150.805398\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 154.604616)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m1f9a9d69ec\" y=\"111.270852\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(7.2 115.070071)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m1f9a9d69ec\" y=\"71.736307\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(7.2 75.535526)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m1f9a9d69ec\" y=\"32.201761\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(7.2 36.00098)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#p6367c0579f)\" d=\"M 45.321307 228.551495 \r\nL 48.395687 228.413383 \r\nL 51.470067 228.260971 \r\nL 54.544447 228.092805 \r\nL 57.618827 227.907289 \r\nL 60.693208 227.70267 \r\nL 63.767588 227.477028 \r\nL 66.841968 227.22826 \r\nL 69.916348 226.954065 \r\nL 72.990728 226.651929 \r\nL 76.065108 226.319105 \r\nL 79.139489 225.952601 \r\nL 82.213869 225.549158 \r\nL 85.288249 225.105235 \r\nL 88.362629 224.616988 \r\nL 91.437009 224.08026 \r\nL 94.511389 223.490556 \r\nL 97.58577 222.843035 \r\nL 100.66015 222.132493 \r\nL 103.73453 221.35336 \r\nL 106.80891 220.499687 \r\nL 109.88329 219.565152 \r\nL 112.95767 218.543062 \r\nL 116.032051 217.426374 \r\nL 119.106431 216.207709 \r\nL 122.180811 214.879395 \r\nL 125.255191 213.433515 \r\nL 128.329571 211.861964 \r\nL 131.403951 210.156537 \r\nL 134.478332 208.309022 \r\nL 137.552712 206.311322 \r\nL 140.627092 204.155592 \r\nL 143.701472 201.834402 \r\nL 146.775852 199.340918 \r\nL 149.850232 196.669105 \r\nL 152.924613 193.813938 \r\nL 155.998993 190.771638 \r\nL 159.073373 187.539906 \r\nL 162.147753 184.118151 \r\nL 165.222133 180.507723 \r\nL 168.296513 176.712104 \r\nL 171.370894 172.737089 \r\nL 174.445274 168.590899 \r\nL 177.519654 164.284261 \r\nL 180.594034 159.830404 \r\nL 183.668414 155.244995 \r\nL 186.742794 150.545984 \r\nL 189.817175 145.75338 \r\nL 192.891555 140.888947 \r\nL 195.965935 135.975829 \r\nL 199.040315 131.038125 \r\nL 202.114695 126.100421 \r\nL 205.189075 121.187303 \r\nL 208.263456 116.32287 \r\nL 211.337836 111.530266 \r\nL 214.412216 106.831255 \r\nL 217.486596 102.245846 \r\nL 220.560976 97.791989 \r\nL 223.635356 93.485351 \r\nL 226.709737 89.339161 \r\nL 229.784117 85.364146 \r\nL 232.858497 81.568527 \r\nL 235.932877 77.958099 \r\nL 239.007257 74.536344 \r\nL 242.081637 71.304612 \r\nL 245.156018 68.262312 \r\nL 248.230398 65.407145 \r\nL 251.304778 62.735332 \r\nL 254.379158 60.241848 \r\nL 257.453538 57.920658 \r\nL 260.527918 55.764928 \r\nL 263.602299 53.767228 \r\nL 266.676679 51.919713 \r\nL 269.751059 50.214286 \r\nL 272.825439 48.642735 \r\nL 275.899819 47.196855 \r\nL 278.974199 45.868541 \r\nL 282.04858 44.649876 \r\nL 285.12296 43.533188 \r\nL 288.19734 42.511098 \r\nL 291.27172 41.576563 \r\nL 294.3461 40.72289 \r\nL 297.42048 39.943757 \r\nL 300.494861 39.233215 \r\nL 303.569241 38.585694 \r\nL 306.643621 37.99599 \r\nL 309.718001 37.459262 \r\nL 312.792381 36.971015 \r\nL 315.866761 36.527092 \r\nL 318.941142 36.123649 \r\nL 322.015522 35.757145 \r\nL 325.089902 35.424321 \r\nL 328.164282 35.122185 \r\nL 331.238662 34.84799 \r\nL 334.313042 34.599222 \r\nL 337.387423 34.37358 \r\nL 340.461803 34.168961 \r\nL 343.536183 33.983445 \r\nL 346.610563 33.815279 \r\nL 349.684943 33.662867 \r\n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p6367c0579f)\" d=\"M 199.040315 32.201761 \r\nL 199.040315 229.874489 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 239.758125 \r\nL 30.103125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 239.758125 \r\nL 364.903125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 239.758125 \r\nL 364.903125 239.758125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 22.318125 \r\nL 364.903125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_12\">\r\n    <!-- Sigmoid Function -->\r\n    <g transform=\"translate(145.801875 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M 53.515625 70.515625 \r\nL 53.515625 60.890625 \r\nQ 47.90625 63.578125 42.921875 64.890625 \r\nQ 37.9375 66.21875 33.296875 66.21875 \r\nQ 25.25 66.21875 20.875 63.09375 \r\nQ 16.5 59.96875 16.5 54.203125 \r\nQ 16.5 49.359375 19.40625 46.890625 \r\nQ 22.3125 44.4375 30.421875 42.921875 \r\nL 36.375 41.703125 \r\nQ 47.40625 39.59375 52.65625 34.296875 \r\nQ 57.90625 29 57.90625 20.125 \r\nQ 57.90625 9.515625 50.796875 4.046875 \r\nQ 43.703125 -1.421875 29.984375 -1.421875 \r\nQ 24.8125 -1.421875 18.96875 -0.25 \r\nQ 13.140625 0.921875 6.890625 3.21875 \r\nL 6.890625 13.375 \r\nQ 12.890625 10.015625 18.65625 8.296875 \r\nQ 24.421875 6.59375 29.984375 6.59375 \r\nQ 38.421875 6.59375 43.015625 9.90625 \r\nQ 47.609375 13.234375 47.609375 19.390625 \r\nQ 47.609375 24.75 44.3125 27.78125 \r\nQ 41.015625 30.8125 33.5 32.328125 \r\nL 27.484375 33.5 \r\nQ 16.453125 35.6875 11.515625 40.375 \r\nQ 6.59375 45.0625 6.59375 53.421875 \r\nQ 6.59375 63.09375 13.40625 68.65625 \r\nQ 20.21875 74.21875 32.171875 74.21875 \r\nQ 37.3125 74.21875 42.625 73.28125 \r\nQ 47.953125 72.359375 53.515625 70.515625 \r\nz\r\n\" id=\"DejaVuSans-83\"/>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n      <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 51.703125 72.90625 \r\nL 51.703125 64.59375 \r\nL 19.671875 64.59375 \r\nL 19.671875 43.109375 \r\nL 48.578125 43.109375 \r\nL 48.578125 34.8125 \r\nL 19.671875 34.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-70\"/>\r\n      <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-83\"/>\r\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"91.259766\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"154.736328\" xlink:href=\"#DejaVuSans-109\"/>\r\n     <use x=\"252.148438\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"313.330078\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"341.113281\" xlink:href=\"#DejaVuSans-100\"/>\r\n     <use x=\"404.589844\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"436.376953\" xlink:href=\"#DejaVuSans-70\"/>\r\n     <use x=\"488.396484\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"551.775391\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"615.154297\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"670.134766\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"709.34375\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"737.126953\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"798.308594\" xlink:href=\"#DejaVuSans-110\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p6367c0579f\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "def sigmoid(x): # 시그모이드 함수 정의\n",
    "    return 1/(1+np.exp(-x))\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.plot(x, y, 'g')\n",
    "plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\n",
    "plt.title('Sigmoid Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x115e4fe5190>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 Cost: 0.693147\n",
      "Epoch  100/1000 Cost: 0.134722\n",
      "Epoch  200/1000 Cost: 0.080643\n",
      "Epoch  300/1000 Cost: 0.057900\n",
      "Epoch  400/1000 Cost: 0.045300\n",
      "Epoch  500/1000 Cost: 0.037261\n",
      "Epoch  600/1000 Cost: 0.031672\n",
      "Epoch  700/1000 Cost: 0.027556\n",
      "Epoch  800/1000 Cost: 0.024394\n",
      "Epoch  900/1000 Cost: 0.021888\n",
      "Epoch 1000/1000 Cost: 0.019852\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱 회귀 구현하기\n",
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "W = torch.zeros((2, 1), requires_grad=True) # 크기는 2 x 1\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b))) # sigmoid\n",
    "    losses = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis))\n",
    "    cost = losses.mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[False],\n        [False],\n        [False],\n        [ True],\n        [ True],\n        [ True]])\ntensor([[3.2530],\n        [1.5179]], requires_grad=True)\ntensor([-14.4819], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction)\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "source": [
    "## 02. `nn.Module`로 구현하는 로지스틱 회귀\n",
    "#### `nn.Linear`, `nn.Sigmoid`\n",
    "#### `nn.Sequential`은 nn.Module 층을 차례로 쌓을 수 있도록 합니다. 이를 이용해 인공 신경망을 구현합니다. 쉽게 말해 여러 함수들을 연결해주는 역할입니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x115e4fe5190>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 Cost: 0.614994 Accuracy 66.67%\n",
      "Epoch   10/1000 Cost: 0.747550 Accuracy 83.33%\n",
      "Epoch   20/1000 Cost: 0.633216 Accuracy 83.33%\n",
      "Epoch   30/1000 Cost: 0.538123 Accuracy 83.33%\n",
      "Epoch   40/1000 Cost: 0.450406 Accuracy 83.33%\n",
      "Epoch   50/1000 Cost: 0.366382 Accuracy 83.33%\n",
      "Epoch   60/1000 Cost: 0.287368 Accuracy 83.33%\n",
      "Epoch   70/1000 Cost: 0.219289 Accuracy 83.33%\n",
      "Epoch   80/1000 Cost: 0.173225 Accuracy 100.00%\n",
      "Epoch   90/1000 Cost: 0.151674 Accuracy 100.00%\n",
      "Epoch  100/1000 Cost: 0.140280 Accuracy 100.00%\n",
      "Epoch  110/1000 Cost: 0.131002 Accuracy 100.00%\n",
      "Epoch  120/1000 Cost: 0.122903 Accuracy 100.00%\n",
      "Epoch  130/1000 Cost: 0.115765 Accuracy 100.00%\n",
      "Epoch  140/1000 Cost: 0.109426 Accuracy 100.00%\n",
      "Epoch  150/1000 Cost: 0.103760 Accuracy 100.00%\n",
      "Epoch  160/1000 Cost: 0.098664 Accuracy 100.00%\n",
      "Epoch  170/1000 Cost: 0.094056 Accuracy 100.00%\n",
      "Epoch  180/1000 Cost: 0.089870 Accuracy 100.00%\n",
      "Epoch  190/1000 Cost: 0.086050 Accuracy 100.00%\n",
      "Epoch  200/1000 Cost: 0.082549 Accuracy 100.00%\n",
      "Epoch  210/1000 Cost: 0.079328 Accuracy 100.00%\n",
      "Epoch  220/1000 Cost: 0.076356 Accuracy 100.00%\n",
      "Epoch  230/1000 Cost: 0.073604 Accuracy 100.00%\n",
      "Epoch  240/1000 Cost: 0.071048 Accuracy 100.00%\n",
      "Epoch  250/1000 Cost: 0.068668 Accuracy 100.00%\n",
      "Epoch  260/1000 Cost: 0.066446 Accuracy 100.00%\n",
      "Epoch  270/1000 Cost: 0.064367 Accuracy 100.00%\n",
      "Epoch  280/1000 Cost: 0.062417 Accuracy 100.00%\n",
      "Epoch  290/1000 Cost: 0.060584 Accuracy 100.00%\n",
      "Epoch  300/1000 Cost: 0.058858 Accuracy 100.00%\n",
      "Epoch  310/1000 Cost: 0.057231 Accuracy 100.00%\n",
      "Epoch  320/1000 Cost: 0.055692 Accuracy 100.00%\n",
      "Epoch  330/1000 Cost: 0.054236 Accuracy 100.00%\n",
      "Epoch  340/1000 Cost: 0.052856 Accuracy 100.00%\n",
      "Epoch  350/1000 Cost: 0.051546 Accuracy 100.00%\n",
      "Epoch  360/1000 Cost: 0.050301 Accuracy 100.00%\n",
      "Epoch  370/1000 Cost: 0.049115 Accuracy 100.00%\n",
      "Epoch  380/1000 Cost: 0.047986 Accuracy 100.00%\n",
      "Epoch  390/1000 Cost: 0.046908 Accuracy 100.00%\n",
      "Epoch  400/1000 Cost: 0.045878 Accuracy 100.00%\n",
      "Epoch  410/1000 Cost: 0.044893 Accuracy 100.00%\n",
      "Epoch  420/1000 Cost: 0.043951 Accuracy 100.00%\n",
      "Epoch  430/1000 Cost: 0.043048 Accuracy 100.00%\n",
      "Epoch  440/1000 Cost: 0.042182 Accuracy 100.00%\n",
      "Epoch  450/1000 Cost: 0.041351 Accuracy 100.00%\n",
      "Epoch  460/1000 Cost: 0.040552 Accuracy 100.00%\n",
      "Epoch  470/1000 Cost: 0.039784 Accuracy 100.00%\n",
      "Epoch  480/1000 Cost: 0.039045 Accuracy 100.00%\n",
      "Epoch  490/1000 Cost: 0.038334 Accuracy 100.00%\n",
      "Epoch  500/1000 Cost: 0.037649 Accuracy 100.00%\n",
      "Epoch  510/1000 Cost: 0.036987 Accuracy 100.00%\n",
      "Epoch  520/1000 Cost: 0.036349 Accuracy 100.00%\n",
      "Epoch  530/1000 Cost: 0.035734 Accuracy 100.00%\n",
      "Epoch  540/1000 Cost: 0.035138 Accuracy 100.00%\n",
      "Epoch  550/1000 Cost: 0.034563 Accuracy 100.00%\n",
      "Epoch  560/1000 Cost: 0.034006 Accuracy 100.00%\n",
      "Epoch  570/1000 Cost: 0.033468 Accuracy 100.00%\n",
      "Epoch  580/1000 Cost: 0.032946 Accuracy 100.00%\n",
      "Epoch  590/1000 Cost: 0.032441 Accuracy 100.00%\n",
      "Epoch  600/1000 Cost: 0.031951 Accuracy 100.00%\n",
      "Epoch  610/1000 Cost: 0.031476 Accuracy 100.00%\n",
      "Epoch  620/1000 Cost: 0.031014 Accuracy 100.00%\n",
      "Epoch  630/1000 Cost: 0.030567 Accuracy 100.00%\n",
      "Epoch  640/1000 Cost: 0.030132 Accuracy 100.00%\n",
      "Epoch  650/1000 Cost: 0.029710 Accuracy 100.00%\n",
      "Epoch  660/1000 Cost: 0.029299 Accuracy 100.00%\n",
      "Epoch  670/1000 Cost: 0.028900 Accuracy 100.00%\n",
      "Epoch  680/1000 Cost: 0.028512 Accuracy 100.00%\n",
      "Epoch  690/1000 Cost: 0.028134 Accuracy 100.00%\n",
      "Epoch  700/1000 Cost: 0.027766 Accuracy 100.00%\n",
      "Epoch  710/1000 Cost: 0.027407 Accuracy 100.00%\n",
      "Epoch  720/1000 Cost: 0.027058 Accuracy 100.00%\n",
      "Epoch  730/1000 Cost: 0.026718 Accuracy 100.00%\n",
      "Epoch  740/1000 Cost: 0.026386 Accuracy 100.00%\n",
      "Epoch  750/1000 Cost: 0.026063 Accuracy 100.00%\n",
      "Epoch  760/1000 Cost: 0.025747 Accuracy 100.00%\n",
      "Epoch  770/1000 Cost: 0.025439 Accuracy 100.00%\n",
      "Epoch  780/1000 Cost: 0.025138 Accuracy 100.00%\n",
      "Epoch  790/1000 Cost: 0.024845 Accuracy 100.00%\n",
      "Epoch  800/1000 Cost: 0.024558 Accuracy 100.00%\n",
      "Epoch  810/1000 Cost: 0.024278 Accuracy 100.00%\n",
      "Epoch  820/1000 Cost: 0.024004 Accuracy 100.00%\n",
      "Epoch  830/1000 Cost: 0.023737 Accuracy 100.00%\n",
      "Epoch  840/1000 Cost: 0.023475 Accuracy 100.00%\n",
      "Epoch  850/1000 Cost: 0.023219 Accuracy 100.00%\n",
      "Epoch  860/1000 Cost: 0.022969 Accuracy 100.00%\n",
      "Epoch  870/1000 Cost: 0.022724 Accuracy 100.00%\n",
      "Epoch  880/1000 Cost: 0.022484 Accuracy 100.00%\n",
      "Epoch  890/1000 Cost: 0.022250 Accuracy 100.00%\n",
      "Epoch  900/1000 Cost: 0.022020 Accuracy 100.00%\n",
      "Epoch  910/1000 Cost: 0.021795 Accuracy 100.00%\n",
      "Epoch  920/1000 Cost: 0.021574 Accuracy 100.00%\n",
      "Epoch  930/1000 Cost: 0.021358 Accuracy 100.00%\n",
      "Epoch  940/1000 Cost: 0.021147 Accuracy 100.00%\n",
      "Epoch  950/1000 Cost: 0.020939 Accuracy 100.00%\n",
      "Epoch  960/1000 Cost: 0.020736 Accuracy 100.00%\n",
      "Epoch  970/1000 Cost: 0.020536 Accuracy 100.00%\n",
      "Epoch  980/1000 Cost: 0.020340 Accuracy 100.00%\n",
      "Epoch  990/1000 Cost: 0.020148 Accuracy 100.00%\n",
      "Epoch 1000/1000 Cost: 0.019960 Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "model = nn.Sequential( \n",
    "   nn.Linear(2, 1), # input_dim = 2, output_dim = 1\n",
    "   nn.Sigmoid() # 출력은 시그모이드 함수를 거친다\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
    "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( \n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))"
   ]
  },
  {
   "source": [
    "## 03. 클래스로 로지스틱 회귀 구현하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x115e4fe5190>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 Cost: 0.614994 Accuracy 66.67%\n",
      "Epoch  100/1000 Cost: 0.140280 Accuracy 100.00%\n",
      "Epoch  200/1000 Cost: 0.082549 Accuracy 100.00%\n",
      "Epoch  300/1000 Cost: 0.058858 Accuracy 100.00%\n",
      "Epoch  400/1000 Cost: 0.045878 Accuracy 100.00%\n",
      "Epoch  500/1000 Cost: 0.037649 Accuracy 100.00%\n",
      "Epoch  600/1000 Cost: 0.031951 Accuracy 100.00%\n",
      "Epoch  700/1000 Cost: 0.027766 Accuracy 100.00%\n",
      "Epoch  800/1000 Cost: 0.024558 Accuracy 100.00%\n",
      "Epoch  900/1000 Cost: 0.022020 Accuracy 100.00%\n",
      "Epoch 1000/1000 Cost: 0.019960 Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "\n",
    "model = BinaryClassifier()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    hypothesis = model(x_train)\n",
    "\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5]) \n",
    "        correct_prediction = prediction.float() == y_train \n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction) \n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( \n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))"
   ]
  },
  {
   "source": [
    "# ------------ 5. 소프트맥스 회귀 ------------- "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}